{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56163412",
   "metadata": {},
   "source": [
    "# 0. Imports and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "643ff690",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T15:36:21.774275Z",
     "start_time": "2021-12-27T15:36:21.769692Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import math\n",
    "\n",
    "pd.options.mode.chained_assignment = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4952863d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T15:36:21.924602Z",
     "start_time": "2021-12-27T15:36:21.918865Z"
    }
   },
   "outputs": [],
   "source": [
    "#The headers for us to look real\n",
    "headers = {'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:95.0) Gecko/20100101 Firefox/95.0'}\n",
    "#The home page\n",
    "home_url = 'https://books.toscrape.com/index.html'\n",
    "#List of categories you wish to scrap \n",
    "cats = ['Classics', 'Science Fiction', 'Humor', 'Business']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf30dce",
   "metadata": {},
   "source": [
    "# 1. Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f603d7",
   "metadata": {},
   "source": [
    "## 1.1 Fetch the Categories Available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45065f59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T15:36:22.348379Z",
     "start_time": "2021-12-27T15:36:22.334085Z"
    }
   },
   "outputs": [],
   "source": [
    "#This function is to create a clean data-set of the categories, their URLs and number of books in each.\n",
    "#This is important to prevent the code from stop working in the case of layout (i.e. order) changes in the webpage.\n",
    "\n",
    "def categories_pages_urls (home_url, headers):\n",
    "    \n",
    "    #Access the page and store it\n",
    "    page = requests.get( home_url, headers=headers )\n",
    "\n",
    "    #Parse the homepage using bs4\n",
    "    soup = BeautifulSoup( page.text, 'html.parser' )\n",
    "\n",
    "    #This is where I found the list including all the categories in the sidebar\n",
    "    categories_list = soup.find( 'ul', class_='nav nav-list' ).find_all('a')\n",
    "\n",
    "    #First, I will create a set with the urls heading to the categories pages.\n",
    "    categories_urls = [p.get('href') for p in categories_list]\n",
    "\n",
    "    #Transform this list in a dataframe, excluding line one which goes for the homepage.\n",
    "    categories_urls_df = pd.DataFrame(categories_urls).iloc[1:]\n",
    "\n",
    "    #Now, I need to get a list of the categories themselves to concatenate with the previous list.\n",
    "    #Using split '\\n' because there was some weird spacing back there in the html\n",
    "    categories_titles = [p.get_text().split('\\n') for p in categories_list]\n",
    "\n",
    "    #Transforming it in a dataframe and dropping alien columns\n",
    "    categories_titles_df = pd.DataFrame(categories_titles).iloc[1:, [False, False, True, False, False]]\n",
    "\n",
    "    #Move the tables together and drop more alien columns\n",
    "    categories_location = pd.concat([categories_titles_df, categories_urls_df], axis = 1).reset_index().iloc[:, [False, True, True]]\n",
    "    categories_location.columns = ['category', 'category_url']\n",
    "\n",
    "    #remove weird 32 spacing from category names\n",
    "    for i in range(len(categories_location)):\n",
    "        categories_location['category'][i] = categories_location['category'][i][32:]\n",
    "\n",
    "    categories_location['books_in_category'] = 0\n",
    "\n",
    "    #Now, I will use a loop to get the quantity of books in each category (we are going to need it later)\n",
    "    for i in range(len(categories_location)):\n",
    "\n",
    "        url_get_cat_number = 'https://books.toscrape.com/'+categories_location.iloc[i]['category_url']\n",
    "        page = requests.get( url_get_cat_number, headers=headers )\n",
    "        soup = BeautifulSoup( page.text, 'html.parser' )\n",
    "        books_in_cat = soup.find( 'form', class_='form-horizontal' ).find('strong')\n",
    "        qty = int([p.get_text('strong') for p in books_in_cat][0])\n",
    "        categories_location.loc[i, 'books_in_category'] = qty\n",
    "        \n",
    "    return categories_location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d243e7f4",
   "metadata": {},
   "source": [
    "## 1.2 Select only the wished categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "796431b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T15:36:22.862260Z",
     "start_time": "2021-12-27T15:36:22.856382Z"
    }
   },
   "outputs": [],
   "source": [
    "#The input to this function (df_cats_urls_qtys) should be the return of categories_pages_urls.\n",
    "#cats should be a list of categories to be scrapped\n",
    "def cats_wish_to_scrap(df_cats_urls_qtys, cats, pagination):\n",
    "    \n",
    "    #Filter\n",
    "    boolean = df_categories_urls.category.isin(cats)\n",
    "    wish = df_categories_urls[boolean]\n",
    "    \n",
    "    #Add pagination info, we will need it later\n",
    "    wish['pages'] = 0\n",
    "    wish['pages'] = wish['books_in_category']/pagination\n",
    "    wish['pages'] = wish['pages'].apply(lambda x: int(math.ceil(x)))\n",
    "    \n",
    "    return wish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bb40b6",
   "metadata": {},
   "source": [
    "## 1.3  Fetch the URLs of every single book in the selected categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d44f2362",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T15:36:23.191553Z",
     "start_time": "2021-12-27T15:36:23.182438Z"
    }
   },
   "outputs": [],
   "source": [
    "#The input to this function should be the return of cats_wish_to_scrap\n",
    "#====== NEED TO ADD PAGINATION FEATURES==========\n",
    "def create_worklist_to_scrap(worklist):\n",
    "    df_worklist = pd.DataFrame()\n",
    "\n",
    "    for i in range(len(worklist)):\n",
    "        #let's start with Classics from worklist\n",
    "        url_titles = 'http://books.toscrape.com/'+ worklist.iloc[i, 1]\n",
    "\n",
    "        #Access the page and store it\n",
    "        page = requests.get( url_titles, headers=headers )\n",
    "\n",
    "        #Parse the catalogue page using the html.parser\n",
    "        soup = BeautifulSoup( page.text, 'html.parser' )\n",
    "\n",
    "        titles = soup.find('div', class_='col-sm-8 col-md-9')\n",
    "        category_is = titles.find_all('h1')\n",
    "        c = category_is[0].get_text()\n",
    "\n",
    "        url_is = titles.find_all('a')\n",
    "        titles_url = set([p.get('href') for p in url_is])\n",
    "        titles_url = list(titles_url)\n",
    "\n",
    "        df_titles_url = pd.DataFrame(titles_url)\n",
    "        df_titles_url['category'] = c\n",
    "\n",
    "        df_worklist = df_worklist.append(df_titles_url, ignore_index=True)\n",
    "        df_titles_url = df_titles_url.iloc[0:0]\n",
    "\n",
    "    df_worklist.columns = ['title_url','category']\n",
    "\n",
    "    for i in range(len(df_worklist)):\n",
    "        df_worklist['title_url'][i] = 'https://books.toscrape.com/catalogue'+df_worklist['title_url'][i][8:]\n",
    "        \n",
    "    return df_worklist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f030c3",
   "metadata": {},
   "source": [
    "## 1.4 Scrap the books pages and append the data in a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c84f37cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T15:36:23.898742Z",
     "start_time": "2021-12-27T15:36:23.886709Z"
    }
   },
   "outputs": [],
   "source": [
    "def books_scrapping(books_to_scrap):\n",
    "    df_books_scrap = pd.DataFrame(columns=['scrap_time','book_title','book_category', 'book_upc', 'book_price', 'book_availability','book_stars'])\n",
    "\n",
    "    #the date the scrapping was held\n",
    "    scrap_time = datetime.now().strftime( '%Y-%m-%d %H:%M:%S' )\n",
    "\n",
    "    for i in range(len(books_to_scrap)):\n",
    "\n",
    "        book_url = books_to_scrap.iloc[0:]['title_url'][i]\n",
    "        \n",
    "        book_category = books_to_scrap.iloc[0:]['category'][i]\n",
    "\n",
    "        #Access the page and store it\n",
    "        page = requests.get( book_url, headers=headers )\n",
    "\n",
    "        #Parse the homepage using bs4\n",
    "        soup = BeautifulSoup( page.text, 'html.parser' )\n",
    "\n",
    "\n",
    "        #the title of the book\n",
    "        book_title = (soup.find('div', class_=\"col-sm-6 product_main\").find_all('h1')[0]).get_text()\n",
    "\n",
    "        #where most the information is:\n",
    "        book_info_table = soup.find('table', class_=\"table table-striped\").find_all('td')\n",
    "\n",
    "        #ua unique identifier for each title\n",
    "        book_upc = (book_info_table[0]).get_text()\n",
    "\n",
    "        #price excl. tax\n",
    "        book_price = (book_info_table[2]).get_text()\n",
    "\n",
    "        #quantity available\n",
    "        book_availability = (book_info_table[5]).get_text()\n",
    "        \n",
    "        #rating\n",
    "        book_rate = (soup.find('div', class_=\"col-sm-6 product_main\")).find_all('p')\n",
    "        book_stars = (book_rate[2].get('class'))[1]\n",
    "        \n",
    "        df_books_scrap.loc[i,'scrap_time'] = scrap_time\n",
    "        df_books_scrap.loc[i,'book_title'] = book_title\n",
    "        df_books_scrap.loc[i,'book_upc'] = book_upc\n",
    "        df_books_scrap.loc[i,'book_price'] = book_price\n",
    "        df_books_scrap.loc[i,'book_availability'] = book_availability\n",
    "        df_books_scrap.loc[i,'book_category'] = book_category\n",
    "        df_books_scrap.loc[i,'book_stars'] = book_stars\n",
    "        \n",
    "    return df_books_scrap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d69563a",
   "metadata": {},
   "source": [
    "## 1.5 Final transformations for a clean delivery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "03c4951c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T16:02:05.451969Z",
     "start_time": "2021-12-27T16:02:05.445959Z"
    }
   },
   "outputs": [],
   "source": [
    "def data_processing(books_scrap):\n",
    "    #Change string stars to numbers\n",
    "    dic = {'One':1, 'Two':2, 'Three':3, 'Four':4, 'Five':5}\n",
    "    books_scrap['book_stars'].replace(dic, inplace=True)\n",
    "\n",
    "    #Change price to number\n",
    "    books_scrap['book_price'] = books_scrap['book_price'].apply(lambda x: x[2:])\n",
    "\n",
    "    #Turn availability data better\n",
    "    books_scrap[['book_in_stock', 'availability']] = books_scrap['book_availability'].str.split('(', 1, expand=True)\n",
    "    books_scrap[['nr_available', 'drop']] = books_scrap['availability'].str.split(' ', 1, expand=True)\n",
    "\n",
    "    books_scrap.drop(columns = ['book_availability', 'availability', 'drop'], axis = 1, inplace=True)\n",
    "    \n",
    "    return books_scrap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2b4295",
   "metadata": {},
   "source": [
    "# 2. Scrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce50cee",
   "metadata": {},
   "source": [
    "## 2.1 Website structure scrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772163db",
   "metadata": {},
   "source": [
    "### 2.1.1 Get the categories in the Sidebar, their URLs and sizes for pagination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fba249d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T15:37:08.673818Z",
     "start_time": "2021-12-27T15:36:27.656983Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Use the defined function to get the clean dataframe of categories\n",
    "df_categories_urls = categories_pages_urls(home_url=home_url, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57ff895c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T15:37:08.682885Z",
     "start_time": "2021-12-27T15:37:08.676424Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Select only the ones we wish to work on and add pagination info\n",
    "worklist = cats_wish_to_scrap(cats=cats, df_cats_urls_qtys=df_categories_urls, pagination=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28abf267",
   "metadata": {},
   "source": [
    "### 2.1.2 Get the titles in each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2da7e2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T15:37:10.660615Z",
     "start_time": "2021-12-27T15:37:08.684682Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "books_to_scrap = create_worklist_to_scrap(worklist=worklist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0b5305",
   "metadata": {},
   "source": [
    "### 2.1.3 Scrap each book page to retrieve the remaining information and build the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cdce8b60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T15:59:50.365533Z",
     "start_time": "2021-12-27T15:59:09.612265Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "books_scrap = books_scrapping(books_to_scrap=books_to_scrap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0404189",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T13:56:59.475130Z",
     "start_time": "2021-12-27T13:56:59.472798Z"
    }
   },
   "source": [
    "### 2.1.4 Transform the data and export the final dataset to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "69767ba9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T16:02:12.351662Z",
     "start_time": "2021-12-27T16:02:12.337149Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_deliver = data_processing(books_scrap=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2aadb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_deliver.to_csv('CoffeeCookies-dataset.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
